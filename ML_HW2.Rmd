---
title: "Machine Learning HW2"
author: "Javier Gutierrez & Andrea Pineda"
date: "1/28/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Data load & Review

```{r}

library(kknn)
library(ggplot2)
library(boot)
library(rpart)
library(rpart.plot)
```

```{r}
download.file(
"https://github.com/ChicagoBoothML/MLClassData/raw/master/UsedCars/UsedCars.csv",
"UsedCars.csv")

cars_data <- read.csv("UsedCars.csv")


download.file("https://raw.githubusercontent.com/ChicagoBoothML/HelpR/master/docv.R", "docv.R")

source("docv.R") 


```

### 2. TRAIN - TEST SPLIT

```{r}
train <- sample.int(nrow(cars_data), replace=FALSE, size = floor(.75*nrow(cars_data)))
train_cars<- cars_data[train, ]
test_cars <- cars_data[-train, ]

```


### 3. OLS MODEL & PLOT

```{r}

ols_cars <- lm(price ~mileage, 
                 data = train_cars)

train_cars$ols <- ols_cars$fitted.values

summary(ols_cars)
```

```{r}


ggplot(train_cars, aes(x = mileage, y = price)) + geom_point(alpha = .5, color = 'blue') + geom_line(aes(x = mileage, y = ols))


```

### 4. POLYNOMIAL MODEL

```{r}

set.seed(17)

pol_n = 10

cv.error =rep(0,pol_n)
for (i in 1:pol_n ){
 glm.fit = glm(price ~ poly(mileage ,i),data=train_cars)
  cv.error[i]=cv.glm(train_cars, glm.fit, K = 5)$delta [1]
}
cv.error

pol_n = 1:10

cv_poly = data.frame(pol_n, cv.error)

##cv_poly[which.min(cv.error)]

ggplot(cv_poly) + geom_line(aes(pol_n, cv.error)) + scale_x_discrete(limits = 1:10)

### 5


```



```{r}

degree = 3
glm.fit_train = glm(price ~ poly (mileage , degree), data = train_cars)

summary(glm.fit_train)



```



```{r}

train_cars$fit <- glm.fit_train$fitted.values

train_cars$fit <- predict(glm.fit_train)


ggplot(train_cars) + geom_point(aes(x = mileage, y = price), alpha = .5, color = 'blue') + geom_line(aes(x = mileage, y = fit))

```

### 5. KNN & REGRESSION TREES


```{r}

## KNN

set.seed(99)

kv = seq(from= 392, to=410, length.out = 10)

cv1 = docvknn(matrix(train_cars$mileage,ncol=1), train_cars$price,kv,nfold=5)

cv1

## CV ERROR PLOT

k_plot = data.frame(kv,cv1)

ggplot(k_plot, aes(x,cv1)) + geom_line()


## OPTIMAL K & FITTED VALUES

kbest = kv[which.min(cv1)]

kbest

kfbest = kknn(price~mileage,train_cars,data.frame(mileage=sort(train_cars$mileage)), k=kbest,kernel = "rectangular")


train_cars$fit_k <- kfbest$fitted.values

## PLOTS

#plot(train_cars$mileage,train_cars$price,cex.lab=1.2)
#lines(sort(train_cars$mileage),kfbest$fitted,col="red",lwd=2,cex.lab=2)


ggplot(train_cars, aes(x = mileage, y = price)) + geom_point(alpha = .5, color = 'blue') + 
  geom_line(aes(x = sort(mileage) , y = fit_k))


```


```{r}

## REGRESSION TREE

tree = rpart(price ~ mileage, data = train_cars, method = "anova", control = rpart.control(minsplit = 10, cp = 0.001)) 

## LEAFS

length(tree$frame$var[tree$frame$var=="<leaf>"])

## OPTIMAL PARAMETER CP

cptable = printcp(tree)
bestcp = cptable[ which.min(cptable[,"xerror"]), "CP" ]
bestcp


## PLOTS

rpart.plot(tree)

plotcp(tree)

## PRUNE BY BEST CP (MINIMUM)

pruned.tree <-prune(tree, cp = bestcp)
length(pruned.tree$frame$var[pruned.tree$frame$var=="<leaf>"])

prp(pruned.tree, type = 1, extra = 1, split.font = 1, varlen = -10)
```
```{r}

## REGRESSION TREE FITTED VALUES AND PLOT

train_cars$fit_tree <- predict(pruned.tree)

ggplot(train_cars, aes(x = mileage, y = price)) + geom_point(alpha = .5, color = 'blue') + geom_line(aes(x = mileage, y = fit_tree))


```

### 6. Multiple Predictors for KNN & REGRESSION TREES

```{r, eval = FALSE}

x = cbind(train_cars$mileage, train_cars$year)
colnames(x) = c("mileage","year")
y = train_cars$price
mmsc=function(x) {return((x-min(x))/(max(x)-min(x)))}
xs = apply(x,2,mmsc) #apply scaling function to each column of x



#plot y vs each x
par(mfrow=c(1,2)) #two plot frames
plot(x[,1],y,xlab="mileage",ylab="price")
plot(x[,2],y,xlab="year",ylab="price")#run cross val once
par(mfrow=c(1,1))
set.seed(99)
kv = seq(from= 50, to=500, length.out = 10) #k values to try
n = length(y)
cvtemp = docvknn(xs,y,kv,nfold=10)
cvtemp = sqrt(cvtemp/n) #docvknn returns sum of squares
plot(kv,cvtemp)


kv_mult = data.frame(kv, cvtemp)

kbest_mult = kv[which.min(cvtemp)]

kbest_mult


## run cross val several times
set.seed(99)
cvmean = rep(0,length(kv)) #will keep average rmse here
ndocv = 10 #number of CV splits to try
n=length(y)
cvmat = matrix(0,length(kv),ndocv) #keep results for each split
for(i in 1:ndocv) {
  cvtemp = docvknn(xs,y,kv,nfold=10)    
  cvmean = cvmean + cvtemp    
  cvmat[,i] = sqrt(cvtemp/n)
  }
cvmean = cvmean/ndocv
cvmean = sqrt(cvmean/n)
plot(kv,cvmean,type="n",ylim=range(cvmat),xlab="k",cex.lab=1.5)
for(i in 1:ndocv) lines(kv,cvmat[,i],col=i,lty=3) #plot each result
lines(kv,cvmean,type="b",col="black",lwd=5) #plot average result
```
```{r}

#refit using all the data and k=100
ddf = data.frame(y,xs)
near5 = kknn(y~.,ddf,ddf,k=100,kernel = "rectangular")
lmf = lm(y~.,ddf)
fmat = cbind(y,near5$fitted,lmf$fitted)
colnames(fmat)=c("y","kNN5","linear")
pairs(fmat)
print(cor(fmat))



#predict price of house in place with lstat=10, indus=11.
x1=100000; 
x2=2005
x1s = (x1-min(x[,1]))/(max(x[,1])-min(x[,1]))
x2s = (x2-min(x[,2]))/(max(x[,2])-min(x[,2]))
near = kknn(y~.,ddf,data.frame(mileage=x1s,year=x2s),k=100,kernel = "rectangular")
cat("knn predicted value: ",near$fitted,"\n")

```





